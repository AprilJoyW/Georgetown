{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Extraction Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Extract each page of the PDF CRCP flyer and convert it to a separage image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fitz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27533/240921394.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfitz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fitz'"
     ]
    }
   ],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"CRCP_brochure.pdf\"\n",
    "doc = fitz.open(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each page in the doc and extract pixel map (sets of pixels defining its color), save as a png file\n",
    "for each in range(len(doc)):\n",
    "    doc.load_page(each).get_pixmap().save(f\"crcp_page{each+1}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Extract the text from the first page of the CRCP flyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv3FeatureExtractor \n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the image of the first page (page 1)\n",
    "image = Image.open(\"crcp_page1.png\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a transformers deep learning model \n",
    "pytorch_text_extractor = LayoutLMv3FeatureExtractor()\n",
    "\n",
    "# Use the model to extract text from the image of the first page of the flyer\n",
    "encoding = pytorch_text_extractor(image, return_tensors=\"pt\") # returns PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding[\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries to make a word cloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wc = WordCloud().generate(\" \".join(encoding[\"words\"][0]))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Extract the text from all pages of the CRCP flyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of documents in working directory (current folder)\n",
    "# that start with \"crcp\", order it by modification time\n",
    "doc_images = sorted(glob.glob(\"crcp*\"), key=os.path.getmtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop through each page and extract the text on the page\n",
    "\n",
    "encodings = []\n",
    "\n",
    "for each in doc_images:\n",
    "    image = Image.open(each).convert(\"RGB\")\n",
    "    pytorch_text_extractor = LayoutLMv3FeatureExtractor()\n",
    "    encodings.append(pytorch_text_extractor(image, return_tensors=\"pt\")) # returns PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOW: 0: first page, 1: second page, etc...\n",
    "encodings[2][\"words\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4) Process extracted text to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text from each page into a single string\n",
    "all_text = [\" \".join(each[\"words\"][0]) for each in encodings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to store all_text, each row is a page in the PDF\n",
    "finra_df = pd.DataFrame({\"page\":range(1,6),\n",
    "                         \"raw_text\":all_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finra_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 Clean text\n",
    "def process_text(t):\n",
    "    import re\n",
    "    \n",
    "    t = t.lower()\n",
    "    t = re.sub(r\"http\\S+\", \"\", t)      # remove links (anything that doesn't have a space after http)\n",
    "    t = re.sub(r\"www.\\S+\", \"\", t)      # remove links\n",
    "    t = re.sub(\"[^a-z]\", \" \", t)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finra_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply process_text function to raw_text column\n",
    "finra_df[\"clean_text\"] = finra_df[\"raw_text\"].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finra_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries to make a word cloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wc = WordCloud(stopwords=STOPWORDS).generate(\" \".join(finra_df[\"clean_text\"]))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import altair as alt\n",
    "\n",
    "# 1 Instantiates a vectorizer to vectorize the text (extract features) based on ngram parameters we specify\n",
    "v = CountVectorizer(stop_words=\"english\",\n",
    "                    ngram_range=(1, 1))\n",
    "    \n",
    "# 2 Vectorizes the text, creates a dataframe with terms and frequencies\n",
    "dtm = v.fit_transform(finra_df[\"clean_text\"])\n",
    "    \n",
    "dtm_df = pd.DataFrame(dtm.toarray(),\n",
    "                     columns=v.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposes the dataframe, make index a column, and renames columns so it is two columns: text, freq\n",
    "dtm_df = dtm_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make term a column not the index\n",
    "dtm_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "dtm_df.columns = [\"term\", \"page1\", \"page2\", \"page3\", \"page4\", \"page5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot words used on each page\n",
    "alt.Chart(dtm_df[dtm_df[\"page3\"] > 0]).mark_bar().encode(alt.Y(\"term\", sort=\"-x\"),\n",
    "                                      x = \"page3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (5) Define a function to adjust the settings/phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crcp_words(df, page, phrase_min, phrase_max):\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    import altair as alt\n",
    "\n",
    "    # 1 Instantiates a vectorizer to vectorize the text (extract features) based on ngram parameters we specify\n",
    "    v = CountVectorizer(stop_words=\"english\",\n",
    "                    ngram_range=(phrase_min, phrase_max))\n",
    "    \n",
    "    # 2 Vectorizes the text, creates a dataframe with terms and frequencies\n",
    "    dtm = v.fit_transform(df[\"clean_text\"])\n",
    "    \n",
    "    dtm_df = pd.DataFrame(dtm.toarray(),\n",
    "                     columns=v.get_feature_names_out())\n",
    "    \n",
    "    dtm_df = dtm_df.T\n",
    "    dtm_df.reset_index(inplace=True)\n",
    "    dtm_df.columns = [\"term\", \"page1\", \"page2\", \"page3\", \"page4\", \"page5\"]\n",
    "    \n",
    "    plot = alt.Chart(dtm_df[dtm_df[f\"{page}\"] > 1]).mark_bar().encode(alt.Y(\"term\", sort=\"-x\"),\n",
    "                                      x = f\"{page}\")\n",
    "    \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function\n",
    "crcp_words(finra_df, page=\"page4\", phrase_min = 3, phrase_max = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Bonus: Extract text from screenshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Took screenshot of SEC site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the image \n",
    "image = Image.open(\"sec_shot.png\").convert(\"RGB\")\n",
    "\n",
    "# Instantiate a transformers deep learning model \n",
    "pytorch_text_extractor = LayoutLMv3FeatureExtractor()\n",
    "\n",
    "# Use the model to extract text from the image of the first page of the site\n",
    "encoding = pytorch_text_extractor(image, return_tensors=\"pt\") # returns PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding[\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
